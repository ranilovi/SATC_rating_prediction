{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Line</th>\n",
       "      <th>date_job</th>\n",
       "      <th>ep_data_name</th>\n",
       "      <th>ep_data_url</th>\n",
       "      <th>ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>S_E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrie</td>\n",
       "      <td>Once upon a time, an English journalist came t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sex and the City</td>\n",
       "      <td>https://www.imdb.com/title/tt0698663/?ref_=tte...</td>\n",
       "      <td>698663</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrie</td>\n",
       "      <td>Elizabeth was attractive and bright.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sex and the City</td>\n",
       "      <td>https://www.imdb.com/title/tt0698663/?ref_=tte...</td>\n",
       "      <td>698663</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrie</td>\n",
       "      <td>Right away she hooked up with one of the city'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sex and the City</td>\n",
       "      <td>https://www.imdb.com/title/tt0698663/?ref_=tte...</td>\n",
       "      <td>698663</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Tim</td>\n",
       "      <td>The question remains-- Is this really a compan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sex and the City</td>\n",
       "      <td>https://www.imdb.com/title/tt0698663/?ref_=tte...</td>\n",
       "      <td>698663</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrie</td>\n",
       "      <td>Tim was 42, a well-liked and respected investm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sex and the City</td>\n",
       "      <td>https://www.imdb.com/title/tt0698663/?ref_=tte...</td>\n",
       "      <td>698663</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  Episode Speaker                                               Line  \\\n",
       "0       1        1  Carrie  Once upon a time, an English journalist came t...   \n",
       "1       1        1  Carrie               Elizabeth was attractive and bright.   \n",
       "2       1        1  Carrie  Right away she hooked up with one of the city'...   \n",
       "3       1        1     Tim  The question remains-- Is this really a compan...   \n",
       "4       1        1  Carrie  Tim was 42, a well-liked and respected investm...   \n",
       "\n",
       "  date_job      ep_data_name  \\\n",
       "0      NaN  Sex and the City   \n",
       "1      NaN  Sex and the City   \n",
       "2      NaN  Sex and the City   \n",
       "3      NaN  Sex and the City   \n",
       "4      NaN  Sex and the City   \n",
       "\n",
       "                                         ep_data_url      ID  Rating  S_E  \n",
       "0  https://www.imdb.com/title/tt0698663/?ref_=tte...  698663     7.4  1_1  \n",
       "1  https://www.imdb.com/title/tt0698663/?ref_=tte...  698663     7.4  1_1  \n",
       "2  https://www.imdb.com/title/tt0698663/?ref_=tte...  698663     7.4  1_1  \n",
       "3  https://www.imdb.com/title/tt0698663/?ref_=tte...  698663     7.4  1_1  \n",
       "4  https://www.imdb.com/title/tt0698663/?ref_=tte...  698663     7.4  1_1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "satc_all_lines_w_rating = pd.read_csv(\"satc_all_lines_w_rating.csv\").drop([\"Unnamed: 0\"],axis=1)\n",
    "satc_all_lines_w_rating['S_E'] = satc_all_lines_w_rating['Season'].astype(int).astype(str)+\"_\"+satc_all_lines_w_rating['Episode'].astype(int).astype(str)\n",
    "satc_all_lines_w_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dos: \n",
    "+ Group all lines of same season + episode together\n",
    "+ preprocess the text, tokenization, remove stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6741e35161e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Distribution of Ratings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Plotting rating distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "# Distribution of Ratings\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting rating distribution\n",
    "X_ratings = satc_all_lines_w_rating.groupby(['S_E']).min().Rating.value_counts()\n",
    "x = sns.barplot(X_ratings.index,X_ratings,palette=\"Set2\")\n",
    "x.set(xlabel='Ratings',ylabel='Frequencies',title='Frequencies of ratings over the {} reviews'.format(satc_all_lines_w_rating.shape[0]))\n",
    "plt.show()\n",
    "#X_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text transformation\n",
    "\n",
    "satc_all_lines_w_rating[\"cleaned\"] = satc_all_lines_w_rating.Line.tolist()\n",
    "#all to lowercase \n",
    "satc_all_lines_w_rating.cleaned = [str(line).lower() for line in satc_all_lines_w_rating.cleaned]\n",
    "\n",
    "#remove special chars\n",
    "chars_remove = [\"@\", \"/\", \"#\", \".\", \",\", \"!\", \"?\", \"(\", \")\", \"-\", \"_\",\"â€™\",\"'\", \"\\\"\", \":\"]\n",
    "trans_dict = {initial:\" \" for initial in chars_remove}\n",
    "satc_all_lines_w_rating.cleaned = [line.translate(str.maketrans(trans_dict)) for line in satc_all_lines_w_rating.cleaned]\n",
    "satc_all_lines_w_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "# NLP library imports\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using NLKT tokenizer to split all text up into individual tokens\n",
    "#we can try different tokenizer here \n",
    "satc_all_lines_w_rating[\"tokenized\"] = [word_tokenize(line) for line in satc_all_lines_w_rating.cleaned]\n",
    "\n",
    "#Removing stopwords for topic extraction using nltk stopwords library\n",
    "stopw = stopwords.words('english')\n",
    "satc_all_lines_w_rating[\"w_o_stopwords\"] = [[token for token in line if token not in stopw] for line in satc_all_lines_w_rating.tokenized]\n",
    "satc_all_lines_w_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#visualisation of the words\n",
    "all_words = []\n",
    "for line in satc_all_lines_w_rating[\"w_o_stopwords\"]:\n",
    "    for word in line:\n",
    "        all_words.append(word)\n",
    "\n",
    "dist = nltk.FreqDist(all_words)\n",
    "X = [nb[1] for nb in dist.most_common(20)]\n",
    "y = [nb[0] for nb in dist.most_common(20)]\n",
    "x = sns.barplot(np.array(X),np.array(y))\n",
    "x.set(xlabel='Word frequencies',ylabel='Words',title='Most common words in the Sex and The City Scripts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#adding oh, nan and na to the list of stopwords, as well as the names of the main characters\n",
    "#main_chars = [\"charlotte\",\"samantha\",\"carrie\",\"miranda\"]\n",
    "new_stopw = stopw + [\"oh\",\"nan\",\"na\"] #+ main_chars\n",
    "satc_all_lines_w_rating[\"w_o_stopwords_2\"] = [[token for token in line if token not in new_stopw] for line in satc_all_lines_w_rating.tokenized]\n",
    "\n",
    "\n",
    "all_words = []\n",
    "for line in satc_all_lines_w_rating[\"w_o_stopwords_2\"]:\n",
    "    for word in line:\n",
    "        all_words.append(word)\n",
    "\n",
    "dist = nltk.FreqDist(all_words)\n",
    "X = [nb[1] for nb in dist.most_common(20)]\n",
    "y = [nb[0] for nb in dist.most_common(20)]\n",
    "x = sns.barplot(np.array(X),np.array(y))\n",
    "x.set(xlabel='Word frequencies',ylabel='Words',title='Most common words in the Sex and The City Scripts, removing further stopwords')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tokenized data set\n",
    "satc_all_lines_w_rating.to_csv(\"satc_all_lines_w_rating_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lemmatization from wordnet in order to merge words that come from the same meaning .- for example, \"friend\" and \"friends\" should be counted as the same word\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tokens = [WordNetLemmatizer().lemmatize(WordNetLemmatizer().lemmatize(WordNetLemmatizer().lemmatize(token,pos='a'),pos='v'),pos='n') for token in tokens]\n",
    "    return tokens  \n",
    "\n",
    "satc_all_lines_w_rating[\"lemmatized\"] = [lemmatize(line) for line in satc_all_lines_w_rating.w_o_stopwords_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the difference in words \n",
    "print(satc_all_lines_w_rating.w_o_stopwords_2[0:10])\n",
    "print(satc_all_lines_w_rating.lemmatized[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for line in satc_all_lines_w_rating[\"lemmatized\"]:\n",
    "    for word in line:\n",
    "        all_words.append(word)\n",
    "\n",
    "dist = nltk.FreqDist(all_words)\n",
    "X = [nb[1] for nb in dist.most_common(20)]\n",
    "y = [nb[0] for nb in dist.most_common(20)]\n",
    "x = sns.barplot(np.array(X),np.array(y))\n",
    "x.set(xlabel='Word frequencies',ylabel='Words',title='Most common words in the Sex and The City Scripts, Lemmatized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Matrix  \n",
    "in order to highlight words specific to an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by episode to define one episode as one document for tf-idf\n",
    "\n",
    "#initizalize new df\n",
    "satc_text_per_episode = pd.DataFrame(\n",
    "    columns=list(satc_all_lines_w_rating)[5:10])\n",
    "\n",
    "#init\n",
    "current_s_e = \"1_1\"\n",
    "all_lines = []\n",
    "\n",
    "for index, row in satc_all_lines_w_rating.iterrows():\n",
    "        if(row.S_E == current_s_e):\n",
    "            #we use the lemmatized version of the lines, change here for different choice of text preprocessing\n",
    "            all_lines.append(row.lemmatized)\n",
    "\n",
    "        if row.S_E != current_s_e:\n",
    "            #flatten all tokens for previous episode and add them to the dataframe\n",
    "            flat_all_lines = [word for line in all_lines for word in line]\n",
    "            #add a new row to new df, taking the information from the index-1 (the last s_e)\n",
    "            satc_text_per_episode = satc_text_per_episode.append({'ep_data_name': satc_all_lines_w_rating.ep_data_name[index-1],\n",
    "                                                                  'ep_data_url': satc_all_lines_w_rating.ep_data_url[index-1],\n",
    "                                                                  'ID': satc_all_lines_w_rating.ID[index-1],\n",
    "                                                                  'Rating': satc_all_lines_w_rating.Rating[index-1],\n",
    "                                                                  'S_E': satc_all_lines_w_rating.S_E[index-1],\n",
    "                                                                  'all_text': flat_all_lines}, ignore_index=True)\n",
    "            #update vars\n",
    "            all_lines = []\n",
    "            current_s_e = row.S_E\n",
    "\n",
    "\n",
    "satc_text_per_episode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create the tfidf matrix\n",
    "vect = TfidfVectorizer(analyzer ='word',ngram_range=(1,1),encoding='latin1')\n",
    "vect_transformed = vect.fit_transform([text for text in satc_text_per_episode['all_text'].astype(str)])\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "satc_tfidf = pd.concat([satc_text_per_episode[['S_E','ep_data_name','ID','Rating']],\n",
    "                        pd.DataFrame(vect_transformed.todense(), columns = feature_names)],axis=1)\n",
    "\n",
    "satc_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some words with smallest and largest tfids\n",
    "sorted_tfidf_index = vect_transformed.max(0).toarray()[0].argsort()\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_features(method ='tf', k=100):\n",
    "    k=k\n",
    "    if (method == 'tfidf'):\n",
    "        #getting topK highest tfidf words\n",
    "        top_k = feature_names[sorted_tfidf_index[:-(k+1):-1]]\n",
    "    elif (method == 'tf'):\n",
    "        #topK most common words\n",
    "        counter = Counter([item for sublist in satc_text_per_episode['all_text'] for item in sublist])\n",
    "        top_k = counter.most_common(k)\n",
    "    else:\n",
    "        top_k = None\n",
    "        print(\"Bad input!! Choose tf or tfidf as first arg\")\n",
    "    \n",
    "    return list(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_words = get_features('tfidf',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lm = LassoCV(n_alphas=100, alphas=[.01,.1,1,10,100]) \n",
    "\n",
    "features_df = satc_tfidf[satc_tfidf.columns[4:]].filter(features_words,axis=1)\n",
    "features = features_df.as_matrix()\n",
    "\n",
    "target_df = satc_tfidf['Rating']\n",
    "target = target_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import sklearn.metrics as metrics \n",
    "\n",
    "cms={}\n",
    "labels = satc_tfidf['Rating'].unique()\n",
    "num_run = 10\n",
    "        \n",
    "for i in range (num_run):\n",
    "       \n",
    "    # separate datasets into training and test datasets once, no folding\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3)\n",
    "\n",
    "    # train the features and target datasets and fit to a model\n",
    "    trained_lm = lm.fit(features_train, target_train)\n",
    "\n",
    "\n",
    "    # predict target with feature test set using trained model\n",
    "    target_pred_train = list(trained_lm.predict(features_train))\n",
    "    target_pred_test = list(trained_lm.predict(features_test))\n",
    "\n",
    "    cms[i]=[metrics.mean_squared_error(target_test, target_pred_test),\n",
    "            metrics.mean_absolute_error (target_test, target_pred_test),\n",
    "            metrics.explained_variance_score(target_test, target_pred_test),\n",
    "            metrics.r2_score(target_test, target_pred_test)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(data=cms,orient='index',columns=['MSE','MAbsE', 'Explained_variance_score','R^2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(target_test, target_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
